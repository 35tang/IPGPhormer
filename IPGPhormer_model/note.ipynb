{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import dgl\n",
    "from dgl import function as fn\n",
    "from dgl._ffi.base import DGLError\n",
    "from dgl.nn.pytorch import edge_softmax\n",
    "from dgl.nn.pytorch.utils import Identity\n",
    "from dgl.utils import expand_as_pair\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "#获取节点序列\n",
    "\n",
    "class HOConv(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, norm='both', num_type=6, weight=True, bias=True):\n",
    "        super(HOConv, self).__init__()\n",
    "        if norm not in ('none', 'both', 'right', 'left'):\n",
    "            raise DGLError('Invalid norm value. Must be either \"none\", \"both\", \"right\" or \"left\".'\n",
    "                           ' But got \"{}\".'.format(norm))\n",
    "        \n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.norm = norm\n",
    "        \n",
    "        #权重、偏置初始化\n",
    "        if weight:\n",
    "            self.weight = nn.Parameter(torch.Tensor(dim_in, dim_out))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(dim_out))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.weight_type = nn.Parameter(torch.ones(num_type))\n",
    "        self.reset_parameters()\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    #重置参数（xavier初始化权重）\n",
    "    def reset_parameters(self):\n",
    "        if self.weight is not None:\n",
    "            init.xavier_uniform_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, G: dgl.DGLGraph):\n",
    "        feat = G.srcdata['feat']        \n",
    "        if self.norm in ['left', 'both']:\n",
    "            degs = G.out_degrees().float().clamp(min=1)     #计算节点出度\n",
    "            if self.norm == 'both':\n",
    "                norm = torch.pow(degs, -0.5)\n",
    "            else:\n",
    "                norm = 1.0 / degs\n",
    "            shp = norm.shape + (1,) * (feat.dim() - 1)\n",
    "            norm = torch.reshape(norm, shp)\n",
    "            feat = feat * norm\n",
    "\n",
    "        G.srcdata['feat'] = feat \n",
    "        G.update_all(fn.copy_src('feat', 'm'), fn.sum(msg='m', out='feat'))\n",
    "        rst = G.dstdata['feat']\n",
    "        rst = torch.matmul(rst, self.weight)\n",
    "\n",
    "        if self.norm in ['right', 'both']:\n",
    "            degs = G.in_degrees().float().clamp(min=1)      #计算节点入度\n",
    "            if self.norm == 'both':\n",
    "                norm = torch.pow(degs, -0.5)\n",
    "            else:\n",
    "                norm = 1.0 / degs\n",
    "            shp = norm.shape + (1,) * (feat.dim() - 1)\n",
    "            norm = torch.reshape(norm, shp)\n",
    "            rst = rst * norm\n",
    "\n",
    "        if self.bias is not None:\n",
    "            rst = rst + self.bias\n",
    "        if self.activation is not None:\n",
    "            rst = self.activation(rst)\n",
    "        \n",
    "        G.dstdata['feat'] = rst\n",
    "        return G\n",
    "\n",
    "#transformer部分！！！(先不动)\n",
    "class AGTLayer(nn.Module):\n",
    "    def __init__(self, dim_in, nheads=2, att_dropout=0.1, emb_dropout=0.1, temper=1.0):\n",
    "        super(AGTLayer, self).__init__()\n",
    "\n",
    "        self.nheads = nheads\n",
    "        self.dim_in = dim_in    #输入维度：dim_in\n",
    "        self.head_dim = self.dim_in // self.nheads\n",
    "\n",
    "        #激活函数\n",
    "        self.leaky = nn.LeakyReLU(0.01)\n",
    "\n",
    "        self.temper = temper\n",
    "        self.linear_q = nn.Linear(\n",
    "            self.dim_in, self.head_dim * self.nheads, bias=False)\n",
    "        self.linear_k = nn.Linear(\n",
    "            self.dim_in, self.head_dim * self.nheads, bias=False)\n",
    "        self.linear_v = nn.Linear(\n",
    "            self.dim_in, self.head_dim * self.nheads, bias=False)\n",
    "\n",
    "        self.linear_final = nn.Linear(self.head_dim * self.nheads, self.dim_in, bias=False)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(att_dropout) #注意力dropout\n",
    "        self.dropout2 = nn.Dropout(emb_dropout) #嵌入dropout\n",
    "        self.LN = nn.LayerNorm(dim_in)\n",
    "\n",
    "    def forward(self, h):\n",
    "        ''' transpose：交换两个维度，适用于二维或更高维的张量。\n",
    "            permute：用于重排多个维度顺序，更灵活适应复杂的维度变换。\n",
    "        '''\n",
    "        batch_size = h.size()[0]\n",
    "        #张量后两位是头数和头维度\n",
    "        k = self.linear_k(h).reshape(batch_size, -1, self.nheads, self.head_dim).transpose(1,2)\n",
    "        q = self.linear_q(h).reshape(batch_size, -1, self.nheads, self.head_dim).permute(0, 2, 3, 1)\n",
    "        v = self.linear_v(h).reshape(batch_size, -1, self.nheads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        score = k @ q\n",
    "        score = score / self.head_dim\n",
    "\n",
    "        score = score / self.temper\n",
    "        score = F.softmax(score, dim=-1)    #行和为1\n",
    "        score = self.dropout1(score)\n",
    "        print(v.shape)\n",
    "        context = score @ v\n",
    "\n",
    "        h_sa = context.transpose(1,2).reshape(batch_size, -1, self.head_dim * self.nheads)\n",
    "        fh = self.linear_final(h_sa)\n",
    "        fh = self.dropout2(fh)\n",
    "\n",
    "        h = self.LN(h + fh)\n",
    "\n",
    "        return h, score\n",
    "\n",
    "class HoGT(nn.Module):\n",
    "    def __init__(self, dim_in=768, dim_hidden=512, num_layers=1, num_gnns=3, nheads=2, dropout=0.1,  temper=1.0, num_type=6):\n",
    "        super(HoGT, self).__init__()\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.num_layers = num_layers\n",
    "        self.num_gnns = num_gnns      \n",
    "        self.nheads = nheads\n",
    "        self.dropout = dropout\n",
    "        self.num_type = num_type\n",
    "\n",
    "        self.fc = nn.Linear(dim_in, dim_hidden)\n",
    "\n",
    "        self.GCNLayers = torch.nn.ModuleList()\n",
    "        self.GTLayers = torch.nn.ModuleList()\n",
    "        for layer in range(self.num_gnns):\n",
    "            self.GCNLayers.append(HOConv(self.dim_hidden, self.dim_hidden))\n",
    "        for layer in range(self.num_layers):\n",
    "            self.GTLayers.append(AGTLayer(self.dim_hidden, self.nheads, self.dropout, self.dropout, temper=temper))\n",
    "        self.Drop = nn.Dropout(dropout)     \n",
    "        \n",
    "        self.fc1 = nn.Sequential(nn.Linear(dim_hidden, dim_hidden), nn.LeakyReLU())\n",
    "    \n",
    "    def forward(self, G:dgl.DGLGraph):\n",
    "        device = G.device\n",
    "        if G.ndata['feat'].shape[-1] != 512:\n",
    "            G.ndata['feat'] = self.fc(G.ndata['feat'])\n",
    "       \n",
    "        for layer in range(self.num_gnns):\n",
    "            G = self.GCNLayers[layer](G)\n",
    "\n",
    "        h = G.ndata['feat'].unsqueeze(0)\n",
    "        h = self.Drop(h)\n",
    "\n",
    "        for layer in range(self.num_layers):\n",
    "            h , global_attn= self.GTLayers[layer](h)\n",
    "            \n",
    "        # print(\"wrong\", h.shape)\n",
    "        G.ndata['feat'] = h.squeeze(0)\n",
    "        global_attn = global_attn.squeeze(0)\n",
    "        global_attn = global_attn.mean(dim = 0)\n",
    "\n",
    "        return G, global_attn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 741, 256])\n",
      "torch.Size([741, 741])\n"
     ]
    }
   ],
   "source": [
    "G_homo = torch.load('/data115_3/TG/Graphdata/graph/STAD/homogeneous/TCGA-3M-AB46-01Z-00-DX1.70F638A0-BDCB-4BDE-BBFE-6D78A1A08C5B.pt')['g']\n",
    "model = HoGT()\n",
    "G, global_attn = model(G_homo)\n",
    "print(global_attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TCGA-A3-3319': {'slide_id': array('TCGA-A3-3319', dtype='<U12'), 'risk': -3.8478031158447266, 'disc_label': 3.0, 'survival': 81.14, 'censorship': 1}, 'TCGA-A3-3323': {'slide_id': array('TCGA-A3-3323', dtype='<U12'), 'risk': -1.1038873195648193, 'disc_label': 3.0, 'survival': 54.43, 'censorship': 0}, 'TCGA-A3-3325': {'slide_id': array('TCGA-A3-3325', dtype='<U12'), 'risk': -3.941983699798584, 'disc_label': 3.0, 'survival': 61.93, 'censorship': 1}, 'TCGA-A3-3329': {'slide_id': array('TCGA-A3-3329', dtype='<U12'), 'risk': -2.0405383110046387, 'disc_label': 2.0, 'survival': 31.27, 'censorship': 0}, 'TCGA-A3-3349': {'slide_id': array('TCGA-A3-3349', dtype='<U12'), 'risk': -1.8341548442840576, 'disc_label': 0.0, 'survival': 5.32, 'censorship': 0}, 'TCGA-A3-3362': {'slide_id': array('TCGA-A3-3362', dtype='<U12'), 'risk': -3.5370707511901855, 'disc_label': 2.0, 'survival': 46.12, 'censorship': 0}, 'TCGA-A3-3372': {'slide_id': array('TCGA-A3-3372', dtype='<U12'), 'risk': -3.9562864303588867, 'disc_label': 1.0, 'survival': 17.35, 'censorship': 1}, 'TCGA-A3-A8OW': {'slide_id': array('TCGA-A3-A8OW', dtype='<U12'), 'risk': -2.0352299213409424, 'disc_label': 1.0, 'survival': 25.79, 'censorship': 1}, 'TCGA-A3-A8OX': {'slide_id': array('TCGA-A3-A8OX', dtype='<U12'), 'risk': -3.0432286262512207, 'disc_label': 3.0, 'survival': 55.29, 'censorship': 1}, 'TCGA-AK-3453': {'slide_id': array('TCGA-AK-3453', dtype='<U12'), 'risk': -3.9646031856536865, 'disc_label': 0.0, 'survival': 10.28, 'censorship': 0}, 'TCGA-AK-3454': {'slide_id': array('TCGA-AK-3454', dtype='<U12'), 'risk': -1.079372763633728, 'disc_label': 2.0, 'survival': 45.04, 'censorship': 0}, 'TCGA-AK-3456': {'slide_id': array('TCGA-AK-3456', dtype='<U12'), 'risk': -3.9715006351470947, 'disc_label': 3.0, 'survival': 48.92, 'censorship': 1}, 'TCGA-AS-3777': {'slide_id': array('TCGA-AS-3777', dtype='<U12'), 'risk': -3.9841179847717285, 'disc_label': 0.0, 'survival': 7.95, 'censorship': 0}, 'TCGA-B0-4698': {'slide_id': array('TCGA-B0-4698', dtype='<U12'), 'risk': -1.2328981161117554, 'disc_label': 3.0, 'survival': 90.47, 'censorship': 1}, 'TCGA-B0-4710': {'slide_id': array('TCGA-B0-4710', dtype='<U12'), 'risk': -1.3000069856643677, 'disc_label': 0.0, 'survival': 1.38, 'censorship': 0}, 'TCGA-B0-4816': {'slide_id': array('TCGA-B0-4816', dtype='<U12'), 'risk': -3.968649387359619, 'disc_label': 3.0, 'survival': 65.6, 'censorship': 1}, 'TCGA-B0-4824': {'slide_id': array('TCGA-B0-4824', dtype='<U12'), 'risk': -1.7243369817733765, 'disc_label': 2.0, 'survival': 46.42, 'censorship': 1}, 'TCGA-B0-4839': {'slide_id': array('TCGA-B0-4839', dtype='<U12'), 'risk': -3.9413931369781494, 'disc_label': 3.0, 'survival': 84.23, 'censorship': 0}, 'TCGA-B0-4842': {'slide_id': array('TCGA-B0-4842', dtype='<U12'), 'risk': -3.969266653060913, 'disc_label': 1.0, 'survival': 23.46, 'censorship': 1}, 'TCGA-B0-4848': {'slide_id': array('TCGA-B0-4848', dtype='<U12'), 'risk': -3.963981866836548, 'disc_label': 1.0, 'survival': 12.25, 'censorship': 1}, 'TCGA-B0-4852': {'slide_id': array('TCGA-B0-4852', dtype='<U12'), 'risk': -3.9138498306274414, 'disc_label': 2.0, 'survival': 31.31, 'censorship': 0}, 'TCGA-B0-5088': {'slide_id': array('TCGA-B0-5088', dtype='<U12'), 'risk': -3.5718376636505127, 'disc_label': 3.0, 'survival': 93.27, 'censorship': 1}, 'TCGA-B0-5094': {'slide_id': array('TCGA-B0-5094', dtype='<U12'), 'risk': -3.9705870151519775, 'disc_label': 2.0, 'survival': 37.22, 'censorship': 1}, 'TCGA-B0-5100': {'slide_id': array('TCGA-B0-5100', dtype='<U12'), 'risk': -1.0319312810897827, 'disc_label': 1.0, 'survival': 10.97, 'censorship': 0}, 'TCGA-B0-5109': {'slide_id': array('TCGA-B0-5109', dtype='<U12'), 'risk': -3.8879642486572266, 'disc_label': 0.0, 'survival': 4.17, 'censorship': 1}, 'TCGA-B0-5110': {'slide_id': array('TCGA-B0-5110', dtype='<U12'), 'risk': -3.9629268646240234, 'disc_label': 0.0, 'survival': 0.95, 'censorship': 1}, 'TCGA-B0-5121': {'slide_id': array('TCGA-B0-5121', dtype='<U12'), 'risk': -3.116133689880371, 'disc_label': 2.0, 'survival': 32.59, 'censorship': 0}, 'TCGA-B0-5696': {'slide_id': array('TCGA-B0-5696', dtype='<U12'), 'risk': -1.9895142316818237, 'disc_label': 1.0, 'survival': 10.94, 'censorship': 0}, 'TCGA-B0-5699': {'slide_id': array('TCGA-B0-5699', dtype='<U12'), 'risk': -3.9710841178894043, 'disc_label': 3.0, 'survival': 49.15, 'censorship': 1}, 'TCGA-B0-5707': {'slide_id': array('TCGA-B0-5707', dtype='<U12'), 'risk': -3.7863588333129883, 'disc_label': 2.0, 'survival': 36.99, 'censorship': 1}, 'TCGA-B0-5709': {'slide_id': array('TCGA-B0-5709', dtype='<U12'), 'risk': -0.21496886014938354, 'disc_label': 1.0, 'survival': 18.43, 'censorship': 0}, 'TCGA-B2-4099': {'slide_id': array('TCGA-B2-4099', dtype='<U12'), 'risk': -3.9765970706939697, 'disc_label': 3.0, 'survival': 53.35, 'censorship': 1}, 'TCGA-B2-4102': {'slide_id': array('TCGA-B2-4102', dtype='<U12'), 'risk': -3.916370391845703, 'disc_label': 3.0, 'survival': 49.05, 'censorship': 0}, 'TCGA-B2-5635': {'slide_id': array('TCGA-B2-5635', dtype='<U12'), 'risk': -3.8563461303710938, 'disc_label': 2.0, 'survival': 45.93, 'censorship': 1}, 'TCGA-B2-A4SR': {'slide_id': array('TCGA-B2-A4SR', dtype='<U12'), 'risk': -2.6692323684692383, 'disc_label': 0.0, 'survival': 9.63, 'censorship': 1}, 'TCGA-B4-5378': {'slide_id': array('TCGA-B4-5378', dtype='<U12'), 'risk': -1.9646000862121582, 'disc_label': 1.0, 'survival': 11.3, 'censorship': 0}, 'TCGA-B4-5838': {'slide_id': array('TCGA-B4-5838', dtype='<U12'), 'risk': -3.958219528198242, 'disc_label': 2.0, 'survival': 31.93, 'censorship': 1}, 'TCGA-B4-5844': {'slide_id': array('TCGA-B4-5844', dtype='<U12'), 'risk': -3.9611265659332275, 'disc_label': 3.0, 'survival': 66.23, 'censorship': 1}, 'TCGA-B8-5162': {'slide_id': array('TCGA-B8-5162', dtype='<U12'), 'risk': -2.7835042476654053, 'disc_label': 3.0, 'survival': 64.13, 'censorship': 1}, 'TCGA-B8-5163': {'slide_id': array('TCGA-B8-5163', dtype='<U12'), 'risk': -3.172090768814087, 'disc_label': 3.0, 'survival': 54.43, 'censorship': 1}, 'TCGA-B8-5165': {'slide_id': array('TCGA-B8-5165', dtype='<U12'), 'risk': -1.9835326671600342, 'disc_label': 3.0, 'survival': 74.11, 'censorship': 0}, 'TCGA-B8-A54J': {'slide_id': array('TCGA-B8-A54J', dtype='<U12'), 'risk': -2.9253106117248535, 'disc_label': 0.0, 'survival': 0.0, 'censorship': 1}, 'TCGA-BP-4159': {'slide_id': array('TCGA-BP-4159', dtype='<U12'), 'risk': -3.979491710662842, 'disc_label': 3.0, 'survival': 51.22, 'censorship': 1}, 'TCGA-BP-4163': {'slide_id': array('TCGA-BP-4163', dtype='<U12'), 'risk': -2.4873692989349365, 'disc_label': 3.0, 'survival': 61.47, 'censorship': 1}, 'TCGA-BP-4164': {'slide_id': array('TCGA-BP-4164', dtype='<U12'), 'risk': -3.6555681228637695, 'disc_label': 1.0, 'survival': 18.5, 'censorship': 0}, 'TCGA-BP-4176': {'slide_id': array('TCGA-BP-4176', dtype='<U12'), 'risk': -0.08600660413503647, 'disc_label': 1.0, 'survival': 19.28, 'censorship': 0}, 'TCGA-BP-4335': {'slide_id': array('TCGA-BP-4335', dtype='<U12'), 'risk': -3.969301462173462, 'disc_label': 3.0, 'survival': 66.95, 'censorship': 1}, 'TCGA-BP-4342': {'slide_id': array('TCGA-BP-4342', dtype='<U12'), 'risk': -2.424361228942871, 'disc_label': 1.0, 'survival': 14.62, 'censorship': 0}, 'TCGA-BP-4345': {'slide_id': array('TCGA-BP-4345', dtype='<U12'), 'risk': -1.0129977464675903, 'disc_label': 1.0, 'survival': 15.6, 'censorship': 0}, 'TCGA-BP-4346': {'slide_id': array('TCGA-BP-4346', dtype='<U12'), 'risk': -2.0237793922424316, 'disc_label': 3.0, 'survival': 49.8, 'censorship': 1}, 'TCGA-BP-4352': {'slide_id': array('TCGA-BP-4352', dtype='<U12'), 'risk': -0.9376689195632935, 'disc_label': 3.0, 'survival': 56.64, 'censorship': 0}, 'TCGA-BP-4355': {'slide_id': array('TCGA-BP-4355', dtype='<U12'), 'risk': -0.03375298157334328, 'disc_label': 3.0, 'survival': 123.0, 'censorship': 1}}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# 打开 .pkl 文件（读取模式为二进制读取 'rb'）\n",
    "with open('/home/ouyangqi/MICCAI_2025/result_baseline/5foldcv/MSGT_nll_surv_a0.0_5foldcv_gc32/tcga_kirc_MSGT_nll_surv_a0.0_5foldcv_gc32_s1/split_latest_val_0_results.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# 查看读取的数据\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入张量的形状: torch.Size([1, 1, 8, 8])\n",
      "输出张量的形状: torch.Size([1, 1, 4, 4])\n",
      "最终输出形状: torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义一个 2x2 的平均池化层\n",
    "class AvgPoolingModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AvgPoolingModel, self).__init__()\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)  # 2x2 池化\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.avg_pool(x)\n",
    "\n",
    "N = 8  # N 必须是 2 的倍数\n",
    "input_tensor = torch.randn(1, 1, N, N)  # 增加 batch 和 channel 维度\n",
    "\n",
    "# 实例化模型\n",
    "model = AvgPoolingModel()\n",
    "\n",
    "# 前向传播\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "print(\"输入张量的形状:\", input_tensor.shape)  # torch.Size([1, 1, 8, 8])\n",
    "print(\"输出张量的形状:\", output_tensor.shape)  # torch.Size([1, 1, 4, 4])\n",
    "\n",
    "# 去掉 batch 和 channel 维度，只保留 (N/2, N/2)\n",
    "output_tensor = output_tensor.squeeze(0).squeeze(0)\n",
    "print(\"最终输出形状:\", output_tensor.shape)  # torch.Size([4, 4])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tangguo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
